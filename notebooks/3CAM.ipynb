{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43f3b7fe",
   "metadata": {},
   "source": [
    "\n",
    "# CAM Tutorial (GoogLeNet, ImageNet)\n",
    "\n",
    "This notebook demonstrates **Class Activation Mapping (CAM)** in the original, simple sense: **convolutions → global average pooling (GAP) → linear → softmax**. We use torchvision.models.googlenet pretrained on ImageNet, which fits the CAM requirement naturally.\n",
    "\n",
    "**What you'll learn** (20 minutes)\n",
    "- Load a pretrained ImageNet model with a GAP head.\n",
    "- Hook the **feature maps before GAP** and read the **linear classifier weights**.\n",
    "- Compute a **class activation map** for the predicted class (or a chosen class).\n",
    "- Overlay the CAM on the input image for visualization.\n",
    "\n",
    "\n",
    "<img src=\"./static/cam.jpg\" alt=\"LO1 Image\" style=\"width: 50%; height: auto;\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08c17256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os # For file path operations\n",
    "import torch # For tensor operations and deep learning\n",
    "import numpy as np # For numerical operations\n",
    "import torch.nn as nn # For neural network layers\n",
    "from PIL import Image # For image processing\n",
    "from torchvision import models, transforms # For pre-trained models and image transformations\n",
    "import torch.nn.functional as F # For functional operations\n",
    "from utils import denormalize, overlay_cam_on_image # For image denormalization and CAM overlay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use CPU by default since no training is being done\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c6871a",
   "metadata": {},
   "source": [
    "### 1. Loading the Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25256fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained GoogLeNet (ImageNet). It already uses conv → GAP → linear and has been trained on ImageNet (1000 classes)\n",
    "googlenet_weights = models.GoogLeNet_Weights.IMAGENET1K_V1 # pretrained weights for GoogLeNet v1\n",
    "model = models.googlenet(weights=googlenet_weights) # load the model\n",
    "model.eval().to(device); # set to evaluation mode and move to device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba3c670",
   "metadata": {},
   "source": [
    "We just loaded this model: GoogLeNet Architecture. Intermediate output predictions are used to counteract the vanishing gradient problem of very deep NNs. Perfect architecture because of Conv into GAP into Softmax.  \n",
    "\n",
    "<img src=\"./static/GoogLeNet.png\" alt=\"LO1 Image\" style=\"width: 90%; height: auto;\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cac5d9d",
   "metadata": {},
   "source": [
    "### 2. Setting Up a \"Hook\" to Capture Feature Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fc9d8d",
   "metadata": {},
   "source": [
    "How do we get the output of an intermediate layer? When we run model(image), we only get the final output. We need a way to \"spy\" on the data as it flows through the network. This is what a PyTorch **hook** is used for. A hook is like a little function you can attach to any layer in your model. When the forward pass happens, your hook function is automatically called, and it gets access to the layer's input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71f0fcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will hook the input to model.avgpool (i.e., the final feature maps before GAP)\n",
    "feature_maps = None\n",
    "\n",
    "# This is our \"spy\" function. PyTorch will automatically pass it three arguments:\n",
    "# module: The layer the hook is attached to (in this case, model.avgpool).\n",
    "# input: A tuple containing the tensor(s) going into the layer.\n",
    "# output: The tensor coming out of the layer.\n",
    "\n",
    "def hook_feature_maps(module, input, output):\n",
    "    # input is a tuple; take the tensor prior to GAP\n",
    "    global feature_maps # The feature_maps variable was created outside the function. To modify it from inside the function, we must tell Python it's a global variable\n",
    "    feature_maps = input[0].detach()  # [B, C, H, W]  Since input is a tuple, we take the first element [0], which is the tensor of feature maps.\n",
    "\n",
    "# This is the line that actually attaches our hook_feature_maps function to the model.avgpool layer. \n",
    "# The hook_handle is an object that we can use later to remove the hook when we're done with it (hook_handle.remove()).\n",
    "hook_handle = model.avgpool.register_forward_hook(hook_feature_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77b6126",
   "metadata": {},
   "source": [
    "### 3. Getting the Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2a6466a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier weight shape: (1000, 1024)\n"
     ]
    }
   ],
   "source": [
    "# The linear classifier weights (shape [num_classes, C])\n",
    "\n",
    "# This directly accesses the weights of the final fully connected (linear) layer, which is named fc in the GoogLeNet model. \n",
    "# This tensor has a shape of (number of classes, number of input features), which for ImageNet is (1000, 1024)\n",
    "fc_weight = model.fc.weight.detach().cpu().numpy()\n",
    "\n",
    " # The googlenet_weights object we created earlier contains metadata, including the human-readable names for all 1000 ImageNet classes\n",
    " # You can look at what the model should be capable of classifying by inspecting these class names.\n",
    "class_names = googlenet_weights.meta.get('categories', None);\n",
    "\n",
    "print('Classifier weight shape:', fc_weight.shape) # here 1024 is the number of input features to the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba8bec3",
   "metadata": {},
   "source": [
    "### 4. Transform the image to be compatible with the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c1538fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing for ImageNet compatibility\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256), # shorter side resized to 256\n",
    "    transforms.CenterCrop(224), # center crop to 224x224\n",
    "    transforms.ToTensor(), # convert PIL image to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225]), # normalize with ImageNet stats\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896df9f4",
   "metadata": {},
   "source": [
    "### 5. CAM computation: \n",
    "$\\text{CAM}_c(x, y) = \\sum_k w_{c,k} * F_k(x, y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129ab194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_maps captured by our hook: (1, C, H, W)\n",
    "# fc_weight: (num_classes, C)\n",
    "# class_idx: int, target class. If None, uses the model's top-1 prediction.\n",
    "\n",
    "def compute_cam_for_class(fmaps: torch.Tensor, class_idx: int, fc_weight: np.ndarray):\n",
    "\n",
    "    # 1. Get the dimensions of the feature maps\n",
    "    C, H, W = # YOUR CODE HERE\n",
    "\n",
    "    # 2. Get the weights for our target class and reshape them for broadcasting\n",
    "    w = # YOUR CODE HERE # (C,1,1)\n",
    "\n",
    "    # 3. Compute the weighted sum of feature maps\n",
    "    # PyTorch will automatically \"stretch\" the 1x1 dimensions of the weights to match the HxW dimensions of the feature maps,\n",
    "    # allowing for a clean element-wise multiplication.\n",
    "    cam = # YOUR CODE HERE    # (H,W) , fmaps[0] because we take the first and only image from the batch\n",
    "\n",
    "    # 4. Apply ReLU to keep only positive contributions\n",
    "    # We are interested in the features that have a positive influence on the classification.\n",
    "    # We want to see the evidence that supports the class prediction, not what contradicts it.\n",
    "    cam = # YOUR CODE HERE\n",
    "    \n",
    "    # 5. Normalize the heatmap to be in the [0, 1] range for visualization\n",
    "    cam = cam - cam.min()\n",
    "    if cam.max() > 0:\n",
    "        cam = cam / cam.max()\n",
    "\n",
    "    return cam.cpu().numpy()  # (H,W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678b4bc4",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>⚠️ Click here for the solution (this will use your \"solution pass\")</summary>\n",
    "  \n",
    "  ```python\n",
    "\n",
    "def compute_cam_for_class(fmaps: torch.Tensor, class_idx: int, fc_weight: np.ndarray):\n",
    "\n",
    "    # 1. Get the dimensions of the feature maps\n",
    "    C, H, W = fmaps.shape[1:] \n",
    "\n",
    "    # 2. Get the weights for our target class and reshape them for broadcasting\n",
    "    w = torch.from_numpy(fc_weight[class_idx]).view(C, 1, 1)  # [C,1,1] \n",
    "\n",
    "    # 3. Compute the weighted sum of feature maps\n",
    "    # PyTorch will automatically \"stretch\" the 1x1 dimensions of the weights to match the HxW dimensions of the feature maps,\n",
    "    # allowing for a clean element-wise multiplication.\n",
    "    cam = torch.sum(fmaps[0] * w.to(fmaps.device), dim=0)     # [H,W] , 0 because we take the first and only image from the batch\n",
    "\n",
    "    # 4. Apply ReLU to keep only positive contributions\n",
    "    # We are interested in the features that have a positive influence on the classification. \n",
    "    # We want to see the evidence that supports the class prediction, not what contradicts it.\n",
    "    cam = F.relu(cam)\n",
    "    \n",
    "    # 5. Normalize the heatmap to be in the [0, 1] range for visualization\n",
    "    cam = cam - cam.min()\n",
    "    if cam.max() > 0:\n",
    "        cam = cam / cam.max()\n",
    "        \n",
    "    return cam.cpu().numpy()  # [H,W] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cace47",
   "metadata": {},
   "source": [
    "### 6. Provide your own image here and calculate the models attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827944b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep images reasonably sized; preprocessing will center-crop to 224x224.\n",
    "path = \"../data/dog.jpg\"\n",
    "\n",
    "# Load and preprocess image\n",
    "orig_img = Image.open(path).convert('RGB')\n",
    "x = preprocess(orig_img).unsqueeze(0).to(device)  # (1,3,224,224)\n",
    "\n",
    "# Forward pass through the model to get predictions (no grad needed)\n",
    "with torch.no_grad():\n",
    "    # n.b., feature_maps is captured by the hook here, since we declared it as global feature_maps now exists\n",
    "    logits =  # YOUR CODE HERE \n",
    "    probs = # YOUR CODE HERE\n",
    "    top_prob, top_idx = probs.max(dim=0) # get the top-1 class and its probability\n",
    "    \n",
    "# Compute CAM for predicted class (top-1)\n",
    "fmap = feature_maps  # [1,C,h,w], thanks to our hook and since we declared it as global feature_maps now exists\n",
    "cam = compute_cam_for_class( YOUR CODE HERE ) # calculation takes in fmaps, indx, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77092a66",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>⚠️ Click here for the solution (this will use your \"solution pass\")</summary>\n",
    "  \n",
    "  ```python\n",
    "# Load and preprocess image\n",
    "orig_img = Image.open(path).convert('RGB')\n",
    "x = preprocess(orig_img).unsqueeze(0).to(device)  # [1,3,224,224]\n",
    "\n",
    "# Forward pass through the model to get predictions (no grad needed)\n",
    "with torch.no_grad():\n",
    "    logits = model(x) # n.b., feature_maps is captured by the hook here, since we declared it as global feature_maps now exists\n",
    "    probs = torch.softmax(logits, dim=1)[0] # [100b0]\n",
    "    top_prob, top_idx = probs.max(dim=0) # get the top-1 class and its probability\n",
    "    \n",
    "# Compute CAM for predicted class (top-1)\n",
    "fmap = feature_maps  # [1,C,h,w], thanks to our hook and since we declared it as global feature_maps now exists\n",
    "cam = compute_cam_for_class(fmap, int(top_idx.item()), fc_weight) # calculation takes in fmaps, indx, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5412e98a",
   "metadata": {},
   "source": [
    "### 7. Visualize result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34142baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_denorm = denormalize(x[0].cpu())\n",
    "overlay = overlay_cam_on_image(x_denorm, cam, alpha=0.7) # n.b., this helper function interpolates the cam back to image size\n",
    "# Titles\n",
    "pred_name = class_names[int(top_idx)] if class_names is not None else str(int(top_idx))\n",
    "print(f\"Top-1: {pred_name} (p={float(top_prob):.3f})\")\n",
    "center_img = Image.fromarray((x_denorm.permute(1,2,0).numpy() * 255).astype('uint8'))\n",
    "\n",
    "# Create a figure with 1 row and 2 columns for our subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 5))\n",
    "# Display the original image on the first subplot (axes[0])\n",
    "axes[0].imshow(center_img)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis('off')  # Hide the x/y axes\n",
    "# Display the CAM overlay on the second subplot (axes[1])\n",
    "axes[1].imshow(overlay)\n",
    "axes[1].set_title(\"CAM Overlay\")\n",
    "axes[1].axis('off')  # Hide the x/y axes\n",
    "# Show the final plot, tight layout and close\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485d1167",
   "metadata": {},
   "source": [
    "### 8. Experiments\n",
    "- Take your own picture or download one from the internet, upload it and then get the saliency map\n",
    "- Remove the ReLu and compare results\n",
    "- Substitute a false target label and explain results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cas_env (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
