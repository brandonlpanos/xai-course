{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18e91a0b",
   "metadata": {},
   "source": [
    "# Local Model-Agnostic Explanations (LIME) for Text\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "1. Load a fine-tuned GPT-2 sentiment classifier.\n",
    "2. Define your example review and tokenization\n",
    "3. Generate perturbed samples of a review in the **interpretable domain** (binary token presence).\n",
    "4. Obtain GPT-2 predictions for each perturbed sample\n",
    "5. Define cosine-distance kernel in binary space\n",
    "6. Build and train the linear surrogate model\n",
    "7. Extract and visualize feature importances\n",
    "8. Compare with the official LIME implementation\n",
    "9. Open question\n",
    "\n",
    "> **Key point**: For text, LIME works in two domains:\n",
    ">\n",
    "> - **Original domain**: token IDs → embeddings → GPT-2 outputs.\n",
    "> - **Interpretable domain**: binary mask vectors indicating which tokens are present.\n",
    ">\n",
    "> We compute **distance** in the interpretable (binary) space, not on embeddings. This is standard practice in text LIME, since the surrogate works on the binary vectors.\n",
    "\n",
    "<img src=\"./static/text_sent.png\" alt=\"LO1 Image\" style=\"width: 90%; height: auto;\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40274962-124a-4fc0-b1cb-edcf3ed19710",
   "metadata": {},
   "source": [
    "### 0. Set up environment and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fc7a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # For machine learning\n",
    "import numpy as np # Array manipulation \n",
    "import math # For generating perturbed data\n",
    "import matplotlib.pyplot as plt # For plotting\n",
    "import torch.nn as nn # machine learning layers\n",
    "from IPython.display import HTML, display  # For display purposes\n",
    "from lime.lime_text import LimeTextExplainer # Inbuilt LIME library for results comparison\n",
    "from torch.utils.data import DataLoader, TensorDataset # for data handling and batch generation\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification # For mapping words to tokens and then to embeddings (model dependent)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # suppress deprecation and trivial warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738dcb80-5bb9-4c1e-9194-347f103b879a",
   "metadata": {},
   "source": [
    "### 1. Load the fine-tuned GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdd9274-15c1-4b41-a983-1596643c5081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"hipnologo/gpt2-imdb-finetune\") # A tokenizer converts text to tokens and is model-dependent\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"hipnologo/gpt2-imdb-finetune\") # A model for text classification fine-tunned on sentiment analysis\n",
    "model.eval() # Set the model to evaluation mode, no dropout or batch normalization\n",
    "\n",
    "# Use GPU for model if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) # Display model details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853c4e6c-0e0c-4ed5-b7e9-fcc75e990a8a",
   "metadata": {},
   "source": [
    "### 2. Define your example review and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec156091-ccdf-448f-a8e0-b8898cf0fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: set a short IMDB-style review string preferably <= 10 words\n",
    "\n",
    "token_ids = tokenizer.encode(review, add_special_tokens=False)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fbcac9-c6dc-4692-8620-95486d49d618",
   "metadata": {},
   "source": [
    "We will use the token **\"the\"** as a neutral token for masking, since it carries minimal sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3492a47-7357-4bf7-b919-7a8a272bb894",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_id = tokenizer.encode(\"the\", add_special_tokens=False)[0]\n",
    "print(f\"Neutral token ID: {neutral_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3143d59-78db-4600-a387-5e60d57776ff",
   "metadata": {},
   "source": [
    "### 3. Generate perturbed samples in the interpretable domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daacf60a-e165-4478-ba41-946e3afb07aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_tokens(token_ids, neutral_id, max_mask_ratio=0.6):\n",
    "    \"\"\"\n",
    "    Generate (sampled) perturbations by randomly masking subsets of tokens.\n",
    "    Returns perturbed token sequences and binary masks (1 = keep, 0 = mask).\n",
    "    \"\"\"\n",
    "    n = len(token_ids)\n",
    "    max_masks = int(n * max_mask_ratio)\n",
    "    masks, sequences = [], []\n",
    "    rng = np.random.default_rng(42)\n",
    "    for mask_count in range(1, max_masks + 1):\n",
    "        total_combos = math.comb(n, mask_count)\n",
    "        samples = min(50, total_combos)\n",
    "        for _ in range(samples):\n",
    "            idxs = rng.choice(n, size=mask_count, replace=False)\n",
    "            mask = np.ones(n, dtype=int)\n",
    "            seq = token_ids.copy()\n",
    "            for i in idxs:\n",
    "                mask[i] = 0\n",
    "                seq[i] = neutral_id\n",
    "            masks.append(mask)\n",
    "            sequences.append(seq)\n",
    "    sequences = torch.tensor(sequences)\n",
    "    masks = torch.tensor(masks)\n",
    "    return sequences, masks\n",
    "\n",
    "Z_ids, Z_masks = perturb_tokens(token_ids, neutral_id)\n",
    "print(\"Perturbations:\", Z_ids.shape, Z_masks.shape)\n",
    "print(Z_ids[100])\n",
    "print(Z_masks[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d839780a-e16e-4873-a04e-dd58edad171d",
   "metadata": {},
   "source": [
    "### 4. Obtain GPT-2 predictions for each perturbed sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f61c67-36b3-4407-9236-0dfb3f1f8518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_probabilities(input_token_ids):\n",
    "    \"\"\"\n",
    "    Takes a batch of token IDs and returns the model's prediction probabilities.\n",
    "\n",
    "    input_token_ids: A tensor of shape (batch_size, sequence_length) containing the token IDs.\n",
    "\n",
    "    Returns: A tensor of shape (batch_size, num_classes) with the probabilities for each class, located on the CPU.\n",
    "    \"\"\"\n",
    "    # We use torch.no_grad() to tell PyTorch we are only doing inference (predicting),\n",
    "    # not training. This makes the code run faster and use less memory.\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # --- Step 1: Move input data to the correct device (e.g., a GPU) ---\n",
    "        # The model and its data must be on the same device to work together.\n",
    "        input_token_ids = input_token_ids.to(device)\n",
    "        \n",
    "        # --- Step 2: Get the raw model outputs (called \"logits\") ---\n",
    "        # We pass the token IDs to the model. The model's output is often an object,\n",
    "        # so we access the raw prediction scores with `.logits`.\n",
    "     \n",
    "        raw_logits =  # YOUR CODE HERE # Shape will be (batch_size, num_classes), e.g., (1, 2)\n",
    "        \n",
    "        # --- Step 3: Convert logits into probabilities ---\n",
    "        # The softmax function turns the raw scores into probabilities that sum to 1.\n",
    "        # `dim=-1` tells softmax to operate on the last dimension (the class scores).\n",
    "        probabilities = # YOUR CODE HERE\n",
    "        \n",
    "        # --- Step 4: Move the result back to the CPU ---\n",
    "        # It's good practice to move data back to the CPU so you can easily\n",
    "        # use it with other libraries like NumPy or Matplotlib.\n",
    "        probabilities_on_cpu = probabilities.cpu()\n",
    "\n",
    "    return probabilities_on_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58b47c0",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>⚠️ Click here for the solution (this will use your \"solution pass\")</summary>\n",
    "  \n",
    "  ```python\n",
    "def get_prediction_probabilities(input_token_ids):\n",
    "    \"\"\"\n",
    "    Takes a batch of token IDs and returns the model's prediction probabilities.\n",
    "    \n",
    "    Args:\n",
    "        input_token_ids (torch.Tensor): A tensor of shape (batch_size, sequence_length)\n",
    "                                        containing the token IDs.\n",
    "                                        \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (batch_size, num_classes) with the probabilities\n",
    "                      for each class, located on the CPU.\n",
    "    \"\"\"\n",
    "    # We use `torch.no_grad()` to tell PyTorch we are only doing inference (predicting),\n",
    "    # not training. This makes the code run faster and use less memory.\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # --- Step 1: Move input data to the correct device (e.g., a GPU) ---\n",
    "        # The model and its data must be on the same device to work together.\n",
    "        input_token_ids = input_token_ids.to(device)\n",
    "        \n",
    "        # --- Step 2: Get the raw model outputs (called \"logits\") ---\n",
    "        # We pass the token IDs to the model. The model's output is often an object,\n",
    "        # so we access the raw prediction scores with `.logits`.\n",
    "        model_outputs = model(input_token_ids)\n",
    "        raw_logits = model_outputs.logits  # Shape will be (batch_size, num_classes), e.g., (1, 2)\n",
    "        \n",
    "        # --- Step 3: Convert logits into probabilities ---\n",
    "        # The softmax function turns the raw scores into probabilities that sum to 1.\n",
    "        # `dim=-1` tells softmax to operate on the last dimension (the class scores).\n",
    "        probabilities = torch.softmax(raw_logits, dim=-1)\n",
    "        \n",
    "        # --- Step 4: Move the result back to the CPU ---\n",
    "        # It's good practice to move data back to the CPU so you can easily\n",
    "        # use it with other libraries like NumPy or Matplotlib.\n",
    "        probabilities_on_cpu = probabilities.cpu()\n",
    "\n",
    "    return probabilities_on_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fce848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch through DataLoader. A DataLoader helps manage large datasets by breaking them into smaller batches. \n",
    "# In this case, we are using it to get prediction probabilities for each batch.\n",
    "dataset = TensorDataset(Z_ids)\n",
    "loader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "probs = [] # List to store probabilities for each batch\n",
    "for (batch_ids,) in loader:\n",
    "    probs.append(get_prediction_probabilities(batch_ids))\n",
    "Y = torch.cat(probs, dim=0)  # shape (num_samples, 2)\n",
    "print(\"Prediction probs:\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3f6339-4293-476b-b7c8-c03010cd5863",
   "metadata": {},
   "source": [
    "### 5. Define cosine-distance kernel in binary space\n",
    "\n",
    "LIME weight for each sample is:\n",
    "\n",
    "$$w_i = \\exp\\Big(-\\frac{(1 - \\cos(x',z'))^2}{\\sigma^2}\\Big)$$\n",
    "\n",
    "where $x'$ is the original binary vector (all ones) and $z'$ is a perturbed mask.\n",
    "\n",
    "> **Note**: We use **distance** = 1 − cosine_similarity, squared in the exponent. Note that cosine means $\\frac{x\\cdot y}{||x|| ||y||}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d27a58-46ce-40a7-8292-0f67a656039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the weight above. This weight is larger for augmentations closer to the original input\n",
    "\n",
    "original_mask = torch.ones_like(Z_masks[0], dtype=torch.float32)  # all tokens present representation of the sentence\n",
    "cos = nn.CosineSimilarity(dim=1) # Can handle batched inputs\n",
    "\n",
    "def kernel_weights(masks, sigma=0.5):\n",
    "    # YOUR CODE HERE \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3923e034",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>⚠️ Click here for the solution (this will use your \"solution pass\")</summary>\n",
    "  \n",
    "  ```python\n",
    "def kernel_weights(masks, sigma=0.5):\n",
    "    # masks: (batch, n_tokens)\n",
    "    dist = 1 - cos(masks.float(), original_mask)\n",
    "    w = torch.exp(-(dist ** 2) / (sigma ** 2))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b8def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "masks = Z_masks[:10]  # take first 10 masks for testing\n",
    "weights = kernel_weights(masks)\n",
    "print(\"Kernel weights:\", weights.shape)\n",
    "# Output should be a tensor of shape Kernel weights: torch.Size([10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba636404-ce78-47d5-874b-26d314812777",
   "metadata": {},
   "source": [
    "### 6. Build and train the linear surrogate model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7b12d8",
   "metadata": {},
   "source": [
    "Step 1: Building Our Simple Surrogate Model.  \n",
    " \n",
    "First, we need to define the structure of our simple model. We'll create a standard PyTorch model that just has one single linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e1b9c-3176-4dbf-9ec9-fbe989d4d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the blueprint for our simple linear model.\n",
    "# It inherits from nn.Module, the base class for all models in PyTorch.\n",
    "class LinearSurrogate(nn.Module):\n",
    "    \n",
    "    # The __init__ method sets up the model's layers.\n",
    "    def __init__(self, num_input_features):\n",
    "        super().__init__() # Always call this first!\n",
    "        # We define a single linear layer.\n",
    "        # It takes num_input_features as input and produces 1 single number as output.\n",
    "        self.linear = nn.Linear(num_input_features, 1)\n",
    "\n",
    "    # The forward method defines what happens when we pass data through the model.\n",
    "    def forward(self, input_data):\n",
    "        # The data just goes through our one linear layer.\n",
    "        return self.linear(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b48016",
   "metadata": {},
   "source": [
    "Step 2: Preparing the Data for Training.  \n",
    " \n",
    "Now, what data do we train this simple model on?\n",
    "- Inputs (X): The binary masks we generated (Z_masks). These represent which features were \"on\" or \"off.\"\n",
    "- Targets (Y): The predictions from our original, complex model (Y).  \n",
    "\n",
    "This is the key trick! We're teaching our simple model to predict what our complex model would predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac6c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pair up our masks (inputs) with the original model's predictions (targets).\n",
    "# This creates our training dataset.\n",
    "training_data = TensorDataset(Z_masks.float(), Y)\n",
    "\n",
    "# The DataLoader will feed this data to our model in small, shuffled batches.\n",
    "# Shuffling helps the model learn better and not get stuck on any ordering in the data.\n",
    "train_loader = DataLoader(training_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf289a3",
   "metadata": {},
   "source": [
    "Step 3: The PyTorch Training Loop Recipe\n",
    "\n",
    "This is the standard \"recipe\" for training almost any model in PyTorch. We'll go through it step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc311aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for Training\n",
    "# 1. Create an instance of our simple model.\n",
    "# The input size is the number of features in our masks.\n",
    "surrogate_model = LinearSurrogate(num_input_features=Z_masks.shape[1])\n",
    "\n",
    "# 2. Choose an optimizer.\n",
    "# The optimizer's job is to adjust the model's weights to reduce the error.\n",
    "# Adam is a popular and effective choice. lr is the learning rate.\n",
    "optimizer = torch.optim.Adam(surrogate_model.parameters(), lr=0.05)\n",
    "\n",
    "\n",
    "# The Training Loop \n",
    "for epoch in range(10):\n",
    "    \n",
    "    total_loss_for_epoch = 0\n",
    "    \n",
    "    # The DataLoader gives us one batch of data at a time.\n",
    "    for masks_batch, predictions_batch in train_loader:\n",
    "        \n",
    "        # --- The 5 Core Steps of a Training Iteration ---\n",
    "        \n",
    "        # 1. PREPARE THE DATA\n",
    "        # Our original model gave probabilities for two classes [prob_class_0, prob_class_1].\n",
    "        # We only want to explain the probability of the positive class (class 1).\n",
    "        true_predictions = predictions_batch[:, 1:2]\n",
    "        \n",
    "        # Calculate weights to focus the model on \"important\" samples (if needed).\n",
    "        # This makes the model pay more attention to masks that are similar to the original input.\n",
    "        sample_weights = kernel_weights(masks_batch, sigma=0.5)\n",
    "\n",
    "        # 2. MAKE A PREDICTION\n",
    "        # Get the surrogate model's prediction for the current batch of masks.\n",
    "        surrogate_prediction = surrogate_model(masks_batch)\n",
    "\n",
    "        # 3. CALCULATE THE LOSS (the error)\n",
    "        # We use a \"Weighted Mean Squared Error\". It measures how far off the prediction is,\n",
    "        # but gives more importance to the samples with higher weights.\n",
    "        # You have everything you need to compose the loss seen in the lecture notes\n",
    "        \n",
    "        loss = # YOUR CODE\n",
    "\n",
    "        # 4. BACKPROPAGATION\n",
    "        # a) Reset previous calculations.\n",
    "        optimizer.zero_grad()\n",
    "        # b) Calculate how to adjust the weights to reduce the loss.\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. UPDATE THE WEIGHTS\n",
    "        # The optimizer takes a small step to improve the model's weights.\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss_for_epoch += loss.item()\n",
    "        \n",
    "    # Print the average loss for this epoch to see if the model is learning.\n",
    "    average_loss = total_loss_for_epoch / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Average Loss = {average_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18470c2f",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>⚠️ Click here for the solution (this will use your \"solution pass\")</summary>\n",
    "  \n",
    "  ```python\n",
    "error = (true_predictions - surrogate_prediction) ** 2\n",
    "weighted_error = sample_weights * error\n",
    "loss = weighted_error.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2c19c3-9bd5-4d13-8c0f-4ab71f426291",
   "metadata": {},
   "source": [
    "### 7. Extract and visualize feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d225825-3392-4023-8703-5d99372b91c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 | Extract raw surrogate weights and corresponding tokens\n",
    "w = surrogate_model.linear.weight.detach().cpu().squeeze().numpy()\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "# 7.2 | Aggregate duplicate tokens by averaging their weights\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "agg = OrderedDict()\n",
    "for tok, weight in zip(tokens, w):\n",
    "    word = tok.lstrip('Ġ')\n",
    "    agg.setdefault(word, []).append(weight)\n",
    "words, vals = zip(*[(word, np.mean(weights)) for word, weights in agg.items()])\n",
    "# 7.3 | Sort by absolute importance (descending)\n",
    "sorted_items = sorted(zip(words, vals), key=lambda x: abs(x[1]), reverse=True)\n",
    "words, vals = zip(*sorted_items)\n",
    "# 7.4 | Plot from most to least important\n",
    "plt.figure(figsize=(8, len(words) * 0.3))\n",
    "colors = ['sandybrown' if v > 0 else 'skyblue' for v in vals]\n",
    "plt.barh(words, vals, color=colors)\n",
    "plt.xlabel('Surrogate weight')\n",
    "plt.title('LIME explanation (interpretable domain) — Most to Least IMPORTANT')\n",
    "plt.gca().invert_yaxis()  # highest importance on top\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21651d6d-2287-4b78-b4e5-77ced58e8a7a",
   "metadata": {},
   "source": [
    "### 8. Compare with the official LIME implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c650b82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_lime(texts):\n",
    "    # returns array of shape (n_samples, 2)\n",
    "    enc = tokenizer(texts, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "    logits = model(**enc).logits\n",
    "    return torch.softmax(logits, -1).detach().cpu().numpy()\n",
    "# Create the explainer and explanation\n",
    "explainer = LimeTextExplainer(class_names=['neg','pos'])\n",
    "exp = explainer.explain_instance(\n",
    "    review,\n",
    "    predict_for_lime,\n",
    "    labels=[1],\n",
    "    num_features=len(token_ids),\n",
    "    num_samples=500)\n",
    "# Render the explanation as HTML and display\n",
    "html = exp.as_html(labels=[1])\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fd9b89-7e32-4a3e-9e31-9cfb8818f382",
   "metadata": {},
   "source": [
    "### 9. Open question\n",
    "\n",
    "**Experiment**: Change `sigma` in the kernel and observe how the explanation weights vary. Can you explain your obsevations?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cas_env (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
