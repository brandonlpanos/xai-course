{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b77466ce",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "**Goal:** Learn how **Decision Trees** work in practice and how to interpret a model. First we will explore simple synthetic data of three variables.  \n",
    "\n",
    "### **Tasks: (10 minutes)**\n",
    "1. Change the seed to generate different data distributions. \n",
    "2. Play with the number of informative and redundant groups. \n",
    "3. I will ask a group to discuss and explain the notebook as well as their findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4771735d",
   "metadata": {},
   "source": [
    "### 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37571711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # For numerical operations\n",
    "import pandas as pd # For data manipulation\n",
    "import matplotlib.pyplot as plt # For plotting\n",
    "from sklearn.tree import DecisionTreeClassifier # For decision tree classification\n",
    "from sklearn.datasets import make_classification # For generating a synthetic dataset\n",
    "from sklearn.model_selection import train_test_split # For splitting data for training\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree # For decision tree visualization\n",
    "np.random.seed(42) # For making results reproducible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babe2865",
   "metadata": {},
   "source": [
    "### 1) Create a simple binary classification dataset with three features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataset with two informative features and one less informative feature\n",
    "X, y = make_classification(n_samples=100, n_features=3, n_informative=2, n_redundant=1, \n",
    "                           n_clusters_per_class=1, n_classes=2, random_state=45)\n",
    "\n",
    "# Convert to a DataFrame for easier manipulation and naming\n",
    "df = pd.DataFrame(X, columns=['Feature1', 'Feature2', 'Feature3'])\n",
    "df['Target'] = y\n",
    "\n",
    "# Make Feature2 binary to represent a color distinction\n",
    "df['Feature2'] = (df['Feature2'] > df['Feature2'].median()).astype(int)\n",
    "\n",
    "# Plot Feature1 vs Feature3 with color indicating the binary Feature2\n",
    "plt.figure(figsize=(7, 5))\n",
    "scatter = plt.scatter(df['Feature1'], df['Feature3'], c=df['Feature2'], cmap='binary', edgecolor='k', s=60)\n",
    "\n",
    "# Add labels and color bar\n",
    "plt.xlabel('Feature1', fontsize=12)\n",
    "plt.ylabel('Feature3', fontsize=12)\n",
    "plt.title('2D Plot of Feature1 vs Feature3 with Feature2 as Binary Color', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b44535",
   "metadata": {},
   "source": [
    "### 2) Train a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54156033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = df[['Feature1', 'Feature2', 'Feature3']]\n",
    "y = df['Target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=41)\n",
    "\n",
    "# Initialize and train the decision tree classifier\n",
    "tree = DecisionTreeClassifier(random_state=42);\n",
    "tree.fit(X_train, y_train);\n",
    "\n",
    "# Quick evaluation\n",
    "pred_val = tree.predict(X_test)\n",
    "print(\"Accuracy on test set:\", int(tree.score(X_test, y_test) * 100),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57971176",
   "metadata": {},
   "source": [
    "### 3) Plot decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe8cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plot_tree(tree, \n",
    "          feature_names=['Feature1', 'Feature2', 'Feature3'], \n",
    "          class_names=['Class 0', 'Class 1'], \n",
    "          filled=False, \n",
    "          rounded=True, \n",
    "          impurity=True, \n",
    "          fontsize=10)\n",
    "plt.title(\"Decision Tree Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645342f7",
   "metadata": {},
   "source": [
    "### 4) Calculate feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e9a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve feature importances\n",
    "feature_importances = tree.feature_importances_ # This is a measure of how important each feature is for the decision tree's predictions.\n",
    "\n",
    "# Display feature importances alongside feature names\n",
    "importance_df = pd.DataFrame({'Feature': ['Feature1', 'Feature2', 'Feature3'],'Importance': feature_importances}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f4b82b",
   "metadata": {},
   "source": [
    "# Partial Dependency and Individual Conditional Expectation Plots\n",
    "\n",
    "In this notebook, you will explore Partial Dependency (PDP) and Individual Conditional Expectation (ICE) plots. You will work with the following dataset and model:\n",
    "\n",
    "**Bike Rental Dataset (bike_sharing_data.csv) and Model (bike_sharing_model.pkl)**  \n",
    "   - This dataset includes four features: `temp`, `feel_temp`, `humidity`, and `windspeed`.  \n",
    "   - Each feature influences the number of bike rentals differently.  \n",
    "   - You will **manually replicate PDP and ICE plots** from `sklearn` to understand these concepts deeply.\n",
    "\n",
    "<img src=\"./static/bike.png\" alt=\"LO1 Image\" style=\"width: 30%; height: auto;\">\n",
    "\n",
    "---\n",
    "\n",
    "### **Tasks: (30 minutes)**\n",
    "- **1.1** | Manually compute and plot the **PDP curve** for the target feature `temp`.\n",
    "- **1.2** | Manually compute and plot the **ICE curves** for `temp`.\n",
    "- **1.3** | **Center** the ICE curves and plot them along with their average.\n",
    "- **1.4** | **Discuss** your results and insights.\n",
    "\n",
    "By the end of this notebook, you should have a strong understanding of how PDP and ICE plots provide insights into model behavior and their limitations in different contexts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b95eb3",
   "metadata": {},
   "source": [
    "### 0) Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d25b075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # For data manipulation\n",
    "import matplotlib.pyplot as plt # For plotting\n",
    "import joblib # For saving and loading models from sklearn\n",
    "from sklearn.inspection import PartialDependenceDisplay # For displaying partial dependence plots and individual conditional expectation\n",
    "import warnings # Filter warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc1ccc1",
   "metadata": {},
   "source": [
    "### 1) Load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39402744",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('../models/bike_sharing_model.pkl')\n",
    "data = pd.read_csv('../data/bike_sharing_data.csv')\n",
    "data.head() # Show the first 5 rows/instances of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec74b686",
   "metadata": {},
   "source": [
    "### 2) Use sklearn's built-in function to plot the ICE and PDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2059a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(8, 5), sharey=True, constrained_layout=True)\n",
    "display = PartialDependenceDisplay.from_estimator(\n",
    "    model, # Pre-trained gradient boosed regressor tree model\n",
    "    data, # Data to plot\n",
    "    features=[\"temp\", \"humidity\", \"windspeed\"], # Features to plot from the dataframe\n",
    "    kind=\"both\",  # This will plot both PDP and ICE\n",
    "    centered=True, # Center the ICE curves becaue we are only interested in the shape of the curve not the absolute values\n",
    "    subsample=50, # Subsample the data to speed up the plotting, we only plot 50 instances\n",
    "    n_jobs=2, # Use 2 cores to speed up the computation\n",
    "    grid_resolution=20, # Number of points in the grid, the higher the better the resolution\n",
    "    random_state=0, # Set the random state for reproducibility\n",
    "    ax=ax) # Plot the ICE and PDP on the same axis\n",
    "display.figure_.suptitle(\"ICE and PDP representations\", fontsize=16) # Set the title of the plot\n",
    "plt.show() # Show the plot\n",
    "plt.close() # Close the plot to free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c96b07c",
   "metadata": {},
   "source": [
    "### 3) Calculate the PDP for a feature of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "426c3cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = \"temp\" # Name of the feature of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db96f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get the sorted, unique values of our feature. This will be the x-axis. (requires one line of code)\n",
    "x_values = # YOUR CODE HERE\n",
    "\n",
    "# 2. Calculate the average prediction for each unique value.\n",
    "pdp_values = [] # Create an empty list to store the PDP values\n",
    "for value in x_values:\n",
    "\n",
    "    # Create a *copy* of the data\n",
    "    data_modified = data.copy()\n",
    "    \n",
    "    # Set the entire feature column to this one value (requires one line of code)\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Get predictions for the modified data\n",
    "    predictions = # YOUR CODE HERE\n",
    "    \n",
    "    # Get the average prediction and add it to our list\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "# Now, pdp_values contains the y-axis of our plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9055b99",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>⚠️ Click here for the solution (this will use your \"solution pass\")</summary>\n",
    "  \n",
    "  ```python\n",
    "# 1. Get the sorted, unique values of our feature. This will be the x-axis. (requires one line of code)\n",
    "x_values = sorted(data[feature_name].unique())\n",
    "\n",
    "# 2. Calculate the average prediction for each unique value.\n",
    "pdp_values = [] # Create an empty list to store the PDP values\n",
    "for value in x_values:\n",
    "\n",
    "    # Create a *copy* of the data\n",
    "    data_modified = data.copy()\n",
    "    \n",
    "    # Set the entire feature column to this one value\n",
    "    data_modified[feature_name] = value\n",
    "    \n",
    "    # Get predictions for the modified data\n",
    "    predictions = model.predict(data_modified)\n",
    "    \n",
    "    # Get the average prediction and add it to our list\n",
    "    pdp_values.append(predictions.mean())\n",
    "\n",
    "# Now, pdp_values contains the y-axis of our plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900a2b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "# Plot the feature values (x-axis) vs. the average predictions (y-axis)\n",
    "plt.plot(x_values, pdp_values, marker='o', linestyle='--', c='darkorange', markersize=2)\n",
    "# Add titles and labels for clarity\n",
    "plt.title(f\"Partial Dependence Plot for '{feature_name}'\")\n",
    "plt.ylabel(\"Average Predicted Outcome\")\n",
    "plt.xlabel(f\"Value of Feature '{feature_name}'\")\n",
    "# Add a grid for easier reading\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10b8c42",
   "metadata": {},
   "source": [
    "### 4) Calculate the ICE curves for 50 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8011689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select a few random instances from the dataset to plot\n",
    "n_ice_samples = 50\n",
    "ice_samples = data.sample(n=n_ice_samples, random_state=42) # Use random_state for reproducibility\n",
    "\n",
    "# 2. Define the grid of feature values (our x-axis, same as for the PDP)\n",
    "x_values = # YOUR CODE HERE\n",
    "\n",
    "# 3. For each sampled instance, calculate its individual prediction curve\n",
    "ice_curves = []\n",
    "for _, instance in ice_samples.iterrows(): # Calculates each ICE curve and stores in array\n",
    "\n",
    "    # Create a new DataFrame by repeating the current instance for each x_value (one line of code) (same shape as you original df but now just filled with repeated same row)\n",
    "    prediction_data = # YOUR CODE HERE\n",
    "    \n",
    "    # Now, replace the feature's column with the range of x_values\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Make predictions. It's good practice to ensure column order is what the model expects.\n",
    "    predictions = # YOUR CODE HERE\n",
    "    \n",
    "    # Add the curve for this instance to our list\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "# Convert the list of curves into a 2D NumPy array for easier plotting\n",
    "ice_curves = np.array(ice_curves)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791136df",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>⚠️ Click here for the solution (this will use your \"solution pass\")</summary>\n",
    "  \n",
    "  ```python\n",
    "# 1. Select a few random instances from the dataset to plot\n",
    "n_ice_samples = 50\n",
    "ice_samples = data.sample(n=n_ice_samples, random_state=42) # Use random_state for reproducibility\n",
    "\n",
    "# 2. Define the grid of feature values (our x-axis, same as for the PDP)\n",
    "x_values = sorted(data[feature_name].unique())\n",
    "\n",
    "# 3. For each sampled instance, calculate its individual prediction curve\n",
    "ice_curves = []\n",
    "for _, instance in ice_samples.iterrows(): \n",
    "\n",
    "    # Create a new DataFrame by repeating the current instance for each x_value\n",
    "    # This is a much simpler way to create the temporary data\n",
    "    prediction_data = pd.DataFrame([instance] * len(x_values))\n",
    "    \n",
    "    # Now, replace the feature's column with the range of x_values\n",
    "    prediction_data[feature_name] = x_values\n",
    "    \n",
    "    # Make predictions. It's good practice to ensure column order is what the model expects.\n",
    "    predictions = model.predict(prediction_data[model.feature_names_in_])\n",
    "    \n",
    "    # Add the curve for this instance to our list\n",
    "    ice_curves.append(predictions)\n",
    "\n",
    "# Convert the list of curves into a 2D NumPy array for easier plotting\n",
    "ice_curves = np.array(ice_curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e523da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "# Plot each curve with some transparency (alpha)\n",
    "for curve in ice_curves:\n",
    "    plt.plot(x_values, curve, color='#1f77b4', alpha=0.1, linewidth=1.0)\n",
    "# Plot the PDP line on top, with a stronger color and line\n",
    "plt.plot(x_values, pdp_values, color='darkorange', linewidth=1.0, marker='o', linestyle='--', markersize=2, label='Partial Dependence (PDP)')\n",
    "# --- Add titles and labels ---\n",
    "plt.title(f\"ICE and PDP for Feature: '{feature_name}'\")\n",
    "plt.xlabel(f\"Value of Feature '{feature_name}'\")\n",
    "plt.ylabel(\"Average Predicted Target\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de231c6c",
   "metadata": {},
   "source": [
    "### 5) Center the ICE curves and plot the results as well as the average on the same axes\n",
    "\n",
    "It is often difficult to interpret ICEs because the plot is unorderly. Since we are only interested in trends and not absolute values, i.e., does the predicted target increase, decrease, or remain the same as we vary the input. One way to center the ICE is to subtract a constant baseline. Selecting the baseline as the predicted value when selecting the anchor as the smallest feature value will ensure all ICE curves start from zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486e081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Choose the anchor point. This is the value where all curves will start.\n",
    "# Let's use the first unique value of the feature, same as before.\n",
    "anchor_value = x_values[0] # Assumes x_values are sorted\n",
    "\n",
    "# 2. Prepare the data for a single, vectorized prediction.\n",
    "# We take our original 50 samples and set the feature column to the anchor value.\n",
    "# The result should be shape (50, number_of_features)\n",
    "anchor_data = ice_samples.copy()\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# 3. Get all anchor predictions in a single call to the model. # Should be (50,)\n",
    "anchor_predictions = # YOUR CODE HERE\n",
    "\n",
    "# 4. Center the ICE curves using NumPy broadcasting.\n",
    "# This subtracts each instance's anchor prediction from its entire curve.\n",
    "# np.newaxis reshapes the predictions array to make the subtraction work correctly.\n",
    "centered_ice_curves = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c8ec66",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>⚠️ Click here for the solution (this will use your \"solution pass\")</summary>\n",
    "  \n",
    "  ```python\n",
    "# 1. Choose the anchor point. This is the value where all curves will start.\n",
    "# Let's use the first unique value of the feature, same as before.\n",
    "anchor_value = x_values[0] # Assumes x_values is sorted\n",
    "\n",
    "# 2. Prepare the data for a single, vectorized prediction.\n",
    "#    We take our original 50 samples and set the feature column to the anchor value.\n",
    "anchor_data = ice_samples.copy()\n",
    "anchor_data[feature_name] = anchor_value  # Should be (50, number_of_features)\n",
    "\n",
    "# 3. Get all anchor predictions in a single call to the model.\n",
    "# This is much faster than predicting in a loop!\n",
    "anchor_predictions = model.predict(anchor_data[model.feature_names_in_])  # Should be (50,)\n",
    "\n",
    "# 4. Center the ICE curves using NumPy broadcasting.\n",
    "# This subtracts each instance's anchor prediction from its entire curve.\n",
    "# np.newaxis reshapes the predictions array to make the subtraction work correctly.\n",
    "centered_ice_curves = ice_curves - anchor_predictions[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d788e167",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "# Plot the centered ICE curves\n",
    "for curve in centered_ice_curves:\n",
    "    plt.plot(x_values, curve, color='#1f77b4', alpha=0.1, linewidth=1.0)\n",
    "# The PDP is the average of the ICE curves, so we can average the centered curves too.\n",
    "centered_pdp = centered_ice_curves.mean(axis=0)\n",
    "plt.plot(x_values, centered_pdp, color='darkorange', linewidth=1.0, linestyle='--', marker='o', markersize=2, label='Centered PDP (c-PDP)')\n",
    "# Add a horizontal line at y=0 to emphasize the anchor point\n",
    "plt.axhline(y=0, color='black', linestyle='--', linewidth=1.5)\n",
    "# Add titles and labels\n",
    "plt.title(f\"Centered ICE Plot for Feature: '{feature_name}\")\n",
    "plt.xlabel(f\"Value of Feature '{feature_name}'\")\n",
    "plt.ylabel(\"Change in Predicted Target (from anchor)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cas_env (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
